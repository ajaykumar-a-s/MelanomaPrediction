{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNN and creation of BSD\n",
    "\n",
    "### This file has the results of the baseline CNN and the creation of BSD dataset. The steps performed are:\n",
    "\n",
    "1. This dataset consists of 2302 training and 998 test samples. \n",
    "2. The experimental dataset is randomly split into train and test sets with a split ratio of 70:30.\n",
    "3. The baseline CNN model has four convolutional layers with 15 filters each of size 3*3. \n",
    "4. Adam's technique is used as the optimizer. Binary cross-entropy is used as the loss function. \n",
    "5. This CNN is trained for 150 epochs. \n",
    "6. Softmax is used as the final activation function which outputs predicted probabilities. \n",
    "7. The BSD is created which contains all the samples whose confidence factor is less than a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import random\n",
    "import imageio\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "import keras.optimizers as optimizers\n",
    "# from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.models import load_model\n",
    "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation, BatchNormalization\n",
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "\n",
    "import imageio.v2 as imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The malignant and benign data is loaded to np arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "def Load_data_malignant():\n",
    "    path =\"Dataset_Final/malignant\"\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(1, 1500):\n",
    "        img = imageio.imread(path +'/' + str(i) + '.jpg')\n",
    "        lab = 1 \n",
    "        x_out.append(img)\n",
    "        y_out.append(lab)\n",
    "    return x_out, y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_data_benign():\n",
    "    path =\"Dataset_Final/benign\"\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(1, 1800):\n",
    "        img = imageio.imread(path +'/' + str(i) + '.jpg')\n",
    "        lab = 0 \n",
    "        x_out.append(img)\n",
    "        y_out.append(lab)\n",
    "    return x_out, y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benign and malignant arrays are assigned to x and y arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, y_0 = Load_data_benign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1, y_1 = Load_data_malignant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array(x_0)\n",
    "y_0 = np.array(y_0)\n",
    "\n",
    "x_1 = np.array(x_1)\n",
    "y_1 = np.array(y_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays are concatenated to form x and y arrays (features, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "x=np.concatenate((x_0, x_1), axis=0)\n",
    "y=np.concatenate((y_0, y_1), axis=0)\n",
    "\n",
    "y = to_categorical(y, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is split as train and test with 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_Test, y_train, y_Test = train_test_split(x, y, test_size=0.3, random_state=5,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN model is defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nClasses = 2\n",
    "\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(15, (3, 3), padding='valid', activation='relu', input_shape=(224,224,3)))\n",
    "    model.add(BatchNormalization())\n",
    "      \n",
    "    model.add(Conv2D(15, (3, 3), padding='same',activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Conv2D(15, (3, 3), padding='same',activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv2D(15, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "    model.summary()\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer is chosen as Adam, binary cross entropy is the loss function, with epochs as 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 15)      420       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 222, 222, 15)      60        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 222, 222, 15)      2040      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 222, 222, 15)      60        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 15)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 111, 111, 15)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 111, 111, 15)      2040      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 111, 111, 15)      60        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 55, 55, 15)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 55, 55, 15)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 53, 53, 15)        2040      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 53, 53, 15)        60        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 15)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10140)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               1014100   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1021082 (3.90 MB)\n",
      "Trainable params: 1020962 (3.89 MB)\n",
      "Non-trainable params: 120 (480.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = None\n",
    "model1 = createModel()\n",
    "batch_size = 40\n",
    "epochs = 150\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99, epsilon=None, decay=0)\n",
    "model1.compile(loss='binary_crossentropy', optimizer=opt,metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/58 [============================>.] - ETA: 3s - loss: 0.6314 - categorical_accuracy: 0.7474"
     ]
    }
   ],
   "source": [
    "history=model1.fit(x_train, y_train, epochs=1,batch_size = batch_size,validation_data= (x_Test, y_Test), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and loss graphs are obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report of the test data is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = model.predict(x_Test, batch_size=32, verbose=1)\n",
    "predicted = np.argmax(pred, axis=1)\n",
    "report = classification_report(np.argmax(y_Test, axis=1), predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing the CPVS of the test samples and the respective outputs, the threshold is calculated as 99.99995 and the BSD is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os \n",
    "for i in range(989):\n",
    "    if abs((pred[i][0]*100) - (pred[i][1]*100)) < 99.99995 :\n",
    "        img = Image.fromarray(x_Test[i], 'RGB')\n",
    "        if np.argmax(y_Test[i])== 0:\n",
    "                img.save(os.path.join('final/benign/',str(i)+'.jpg'))\n",
    "        if np.argmax(y_Test[i])== 1:\n",
    "                img.save(os.path.join('final/malignant/', str(i)+'.jpg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
